(source DATA_PATH.sh && CUDA_VISIBLE_DEVICES=4  python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/caltech101  --dataset caltech101 --num-classes 102  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-2 --warmup-lr 1e-7 --warmup-epochs 10  --lr 1e-3 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/caltech101/ssf --amp  --tuning-mode ssf --pretrained   	--pin-mem \ && ) > param/caltech101.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/cifar/  --dataset cifar100 --num-classes 100  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop 0.05 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/cifar_100/ssf --amp  --tuning-mode ssf --pretrained   	--pin-mem \ && ) > param/cifar_100.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/clevr_count  --dataset clevr_count --num-classes 8  --no-aug  --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-2 --warmup-lr 1e-7 --warmup-epochs 10  --lr 2e-3 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/clevr_count/ssf --amp --tuning-mode ssf --pretrained   --pin-mem \ &&) > param/clevr_count.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/clevr_dist  --dataset clevr_dist --num-classes 6  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-2 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-2 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/clevr_dist/ssf --amp --tuning-mode ssf --pretrained   --pin-mem \ &&) > param/clevr_dist.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/diabetic_retinopathy  --dataset diabetic_retinopathy --num-classes 5  --no-aug --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0.2 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/diabetic_retinopathy/ssf --amp --tuning-mode ssf --pretrained   	--pin-mem \ && ) > param/diabetic_retinopathy.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/dmlab  --dataset dmlab --num-classes 6  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 200 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/dmlab/ssf --amp --tuning-mode ssf --pretrained  --pin-mem \ && ) > param/dmlab.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/dsprites_loc  --dataset dsprites_loc --num-classes 16  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 1e-2 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/dsprites_loc/ssf --amp --tuning-mode ssf --pretrained 	--pin-mem \ && ) > param/dsprites_loc.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/dsprites_ori  --dataset dsprites_ori --num-classes 16  --no-aug   --direct-resize   --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0.2 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/dsprites_ori/ssf --amp --tuning-mode ssf --pretrained 	--pin-mem \ && ) > param/dsprites_ori.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/dtd  --dataset dtd --num-classes 47  --no-aug --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0 --img-size 224 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/dtd/ssf --amp --tuning-mode ssf --pretrained  --mixup 0 --cutmix 0 --smoothing 0 --pin-mem \ &&) > param/dtd.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/eurosat  --dataset eurosat --num-classes 10  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-2 --warmup-lr 1e-7 --warmup-epochs 10  --lr 3e-3 --min-lr 1e-8 --drop-path 0.2 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/eurosat/ssf --amp --tuning-mode ssf --pretrained   	--pin-mem \ && ) > param/eurosat.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/oxford_flowers102 --dataset flowers102 --num-classes 102  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 200 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/flowers102/ssf --amp --tuning-mode ssf --pretrained   	--pin-mem \ &&  && ) > param/flowers102.txt
(   python  -m torch.distributed.launch --nproc_per_node=1 --master_port=10454  train.py ${VTAB_PATH}/kitti  --dataset kitti --num-classes 4  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 1e-2 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/kitti/ssf --amp --tuning-mode ssf --pretrained  --pin-mem \ && ) > param/kitti.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/patch_camelyon  --dataset patch_camelyon --num-classes 2  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/patch_camelyon/ssf --amp --tuning-mode ssf --pretrained   	--pin-mem \ && ) > param/patch_camelyon.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/oxford_iiit_pet  --dataset pets --num-classes 37 --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 300 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop 0.1 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/pets/ssf --amp --tuning-mode ssf --pretrained  --pin-mem \ && ) > param/pets.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/resisc45  --dataset resisc45 --num-classes 45  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 2e-3 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/resisc45/ssf --amp --tuning-mode ssf --pretrained  --pin-mem \ && ) > param/resisc45.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/smallnorb_azi  --dataset smallnorb_azi --num-classes 18  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 200 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 2e-2 --min-lr 1e-8 --drop-path 0.1 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/smallnorb_azi/ssf --amp --tuning-mode ssf --pretrained --pin-mem \ &&  ) > param/smallnorb_azi.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/smallnorb_ele  --dataset smallnorb_ele --num-classes 9  --no-aug  --direct-resize  --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-2 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0.2 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/smallnorb_ele/ssf --amp --tuning-mode ssf --pretrained --pin-mem \ &&  ) > param/smallnorb_ele.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454 train.py ${VTAB_PATH}/sun397  --dataset sun397 --num-classes 397  --no-aug --direct-resize  --model vit_base_patch16_224_in21k  --batch-size 128 --epochs 200 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 5e-3 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_base_patch16_224_in21k/vtab/sun397/ssf --amp --tuning-mode ssf --pretrained   --pin-mem \ && ) > param/sun397.txt
(   python  -m torch.distributed.launch --nproc_per_node=1  --master_port=10454  train.py ${VTAB_PATH}/svhn  --dataset svhn --num-classes 10  --no-aug --direct-resize --model vit_small_patch16_224_in21k  --batch-size 128 --epochs 250 --opt adamw  --weight-decay 5e-5 --warmup-lr 1e-7 --warmup-epochs 10  --lr 1e-2 --min-lr 1e-8 --drop-path 0 --img-size 224 --mixup 0 --cutmix 0 --smoothing 0 --output  ${OUTPUT_PATH}/vit_small_patch16_224_in21k/vtab/svhn/ssf --amp --tuning-mode ssf --pretrained   --pin-mem \ &&) > param/svhn.txt
